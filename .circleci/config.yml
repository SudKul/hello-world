# Use the latest 2.1 version of CircleCI pipeline process engine. See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1
# Use a package of configuration called an orb.
orbs:
  # Choose either one of the orbs below
  # Declare a dependency on the welcome-orb
  # welcome: circleci/welcome-orb@0.4.1
  aws-cli: circleci/aws-cli@2.0.3
# Orchestrate or schedule a set of jobs

commands:
  print_pipeline_id:
    parameters:
      id: 
        type: string
    steps:
      - run: echo << parameters.id >>

  destroy_environment:
    steps:
      - run:
          name: Destroy environment
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable 
          # ${CIRCLE_WORKFLOW_ID:0:5} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID 
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5}
          


jobs:
  print_greetings:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - print_pipeline_id:
          id: << pipeline.id >>
      - run: echo HELLO
      - run: echo WORLD
      - run: echo $_env_name

  upload_file:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - run: echo "THIS IS A SAMPLE TEXT" > ~/output.txt  
      - persist_to_workspace:
          root: ~/
          paths: 
            - output.txt  

  download_file:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - attach_workspace:
          at: ~/
      - run: cat ~/output.txt 
  
  # Exercise - Rollback
  # Job 1 
  create_infrastructure: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file template.yml \
              --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5} \
              --region us-east-1
      # Fail the job intentionally to simulate an error.
      # Uncomment the line below if you want to fail the current step
      # - run: return 1
      - destroy_environment

  # Exercise - Rollback
  # Job 2
  configure_infrastructure: 
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints: ["0c:55:f7:d7:ad:e3:73:19:0b:e6:c6:46:5c:04:76:94"] # You can get this ID in the section where you registered the SSH Key
      - run:
          name: Install dependencies
          command: |
            # install the dependencies needed for your playbook
            apk add --update ansible 
      - run:
          name: Configure server
          command: |
            ansible-playbook -i inventory.txt main4.yml

  # Exercise - Rollback
  # Job 3          
  smoke_test:
    docker:
      - image: alpine:latest
    steps:
      - run: apk add --update curl
      - run:
          name: smoke test
          command: |
            URL="https://blog.udacity.com/"
            # Test if website exists
            if curl -s --head ${URL} 
            then
              return 0
            else
              return 1
            fi
      - destroy_environment      
  
  # Exercise: Promote to Production
  # Prerequisite: An S3 bucket (say `mybucket644752792305`) with a sample index.html created manually in your AWS console. 
  # Run the command below to create a CloudFront Distribution that will connect to the existing bucket. 
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the cloudfront.yml template file.
  # We are assuming that the `PipelineID` parameter represents the bucket name. 

  # aws cloudformation deploy \
  # --template-file cloudfront.yml \
  # --stack-name production-distro \
  # --parameter-overrides PipelineID="mybucket644752792305" \ # Name of the S3 bucket you created manually.
  
  # Job 1 
  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.yml template file.
  create_and_deploy_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name "${CIRCLE_WORKFLOW_ID:0:7}" \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      - run: aws s3 sync . s3://"${CIRCLE_WORKFLOW_ID:0:7}" --delete
  
  # Exercise: Promote to Production
  # Job 2
  # Fetch and save the pipeline/deployment ID responsible for the last release.
  get_last_deployment_id:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Fetch and save the pipeline ID was responsible for the last successful production release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text
  
  # Exercise: Promote to Production
  # Job 3
  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket. 
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
  promote_to_production:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="${CIRCLE_WORKFLOW_ID:0:7}" 

  # Exercise: Promote to Production
  # Job 4
  # Destroy the previous production version's S3 bucket and CloudFormation stack. 
  clean_up_old_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Destroy the previous production version's S3 bucket and CloudFormation stack. 
          command: |
            aws s3 rm "s3://${OldPipelineID}" --recursive
            aws cloudformation delete-stack --stack-name "${PreviousPipelineID}" 

workflows:
  # Name the workflow "welcome"
  my_workflow:
    # Run the welcome/run job in its own container
    jobs:
      # - welcome/run
      - print_greetings
      - upload_file
      - download_file:
          requires:
            - upload_file
      - create_infrastructure
      - configure_infrastructure
      - create_and_deploy_front_end
      - promote_to_production:
          requires: 
            - create_and_deploy_front_end